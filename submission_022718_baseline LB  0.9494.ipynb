{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "sample_submission.csv.zip\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n",
      "toxic            15294\n",
      "severe_toxic      1595\n",
      "obscene           8449\n",
      "threat             478\n",
      "insult            7877\n",
      "identity_hate     1405\n",
      "dtype: int64\n",
      "toxic            0.095844\n",
      "severe_toxic     0.009996\n",
      "obscene          0.052948\n",
      "threat           0.002996\n",
      "insult           0.049364\n",
      "identity_hate    0.008805\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input/train.csv\")\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "print(data[['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']].sum())\n",
    "print(data[['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127572, 8)\n",
      "(31999, 8)\n"
     ]
    }
   ],
   "source": [
    "# Create train and Test Datasets\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic            12268\n",
      "severe_toxic      1278\n",
      "obscene           6762\n",
      "threat             394\n",
      "insult            6334\n",
      "identity_hate     1125\n",
      "dtype: int64\n",
      "toxic            0.096165\n",
      "severe_toxic     0.010018\n",
      "obscene          0.053005\n",
      "threat           0.003088\n",
      "insult           0.049650\n",
      "identity_hate    0.008819\n",
      "dtype: float64\n",
      "toxic            3026\n",
      "severe_toxic      317\n",
      "obscene          1687\n",
      "threat             84\n",
      "insult           1543\n",
      "identity_hate     280\n",
      "dtype: int64\n",
      "toxic            0.094565\n",
      "severe_toxic     0.009907\n",
      "obscene          0.052720\n",
      "threat           0.002625\n",
      "insult           0.048220\n",
      "identity_hate    0.008750\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check that mean of th train and test set is comparable to the global population\n",
    "print(train[['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']].sum())\n",
    "print(train[['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']].mean())\n",
    "print(test[['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']].sum())\n",
    "print(test[['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Baseline ROC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_vec(df, column_name):\n",
    "    return df[[column_name]].as_matrix().reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_score(df=None, column_name=None, prob_vec=None):\n",
    "    y = get_y_vec(df, column_name)\n",
    "    return roc_auc_score(y, prob_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score: 0.506340337971\n",
      "0.99009344042\n"
     ]
    }
   ],
   "source": [
    "#random guessing gets you a ROC of 0.5\n",
    "print('ROC Score:', compute_roc_score(test, 'toxic', np.random.random((31999,))))\n",
    "print((get_y_vec(test, 'severe_toxic') == np.zeros((31999,))).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes classifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function to get balanced samples\n",
    "def build_balanced_classifier(train, test = None, label = None):\n",
    "    y_ = get_y_vec(df = train, column_name=label)\n",
    "    pos_cases = (y_ == 1)\n",
    "    num_pos_cases = pos_cases.sum()\n",
    "    idx_desc = np.argsort(-y_) #sort indices high to low so positive cases are at the front\n",
    "    balance_samples = idx_desc[:2*num_pos_cases] #extract 2x the num pos cases so now 50/50 pos/neg\n",
    "    print(y_[balance_samples].mean()) #confirm probability\n",
    "    y_train_bal = y_[balance_samples]\n",
    "    X_train_bal = train[['comment_text']].as_matrix().reshape((-1,))\n",
    "    X_train_bal = X_train_bal[balance_samples]\n",
    "    print(y_train_bal.shape, X_train_bal.shape)\n",
    "    \n",
    "    y_test, X_test = None, None\n",
    "    if test is not None:\n",
    "        y_test = get_y_vec(df = test, column_name=label)\n",
    "        X_test = test[['comment_text']].as_matrix().reshape((-1,))    \n",
    "    return y_train_bal, X_train_bal, y_test, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### toxic ####################\n",
      "\n",
      "0.5\n",
      "(24536,) (24536,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.91      0.94     28973\n",
      "          1       0.49      0.86      0.62      3026\n",
      "\n",
      "avg / total       0.94      0.90      0.91     31999\n",
      "\n",
      "[[26226  2747]\n",
      " [  432  2594]]\n",
      "0.948275195205\n",
      "########################################\n",
      "\n",
      "########### severe_toxic ####################\n",
      "\n",
      "0.5\n",
      "(2556,) (2556,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97     31682\n",
      "          1       0.13      0.94      0.24       317\n",
      "\n",
      "avg / total       0.99      0.94      0.96     31999\n",
      "\n",
      "[[29773  1909]\n",
      " [   20   297]]\n",
      "0.978573748551\n",
      "########################################\n",
      "\n",
      "########### obscene ####################\n",
      "\n",
      "0.5\n",
      "(13524,) (13524,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.91      0.95     30312\n",
      "          1       0.35      0.86      0.50      1687\n",
      "\n",
      "avg / total       0.96      0.91      0.93     31999\n",
      "\n",
      "[[27655  2657]\n",
      " [  242  1445]]\n",
      "0.949503957498\n",
      "########################################\n",
      "\n",
      "########### threat ####################\n",
      "\n",
      "0.5\n",
      "(788,) (788,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95     31915\n",
      "          1       0.03      0.90      0.05        84\n",
      "\n",
      "avg / total       1.00      0.91      0.95     31999\n",
      "\n",
      "[[29006  2909]\n",
      " [    8    76]]\n",
      "0.975912393784\n",
      "########################################\n",
      "\n",
      "########### insult ####################\n",
      "\n",
      "0.5\n",
      "(12668,) (12668,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.91      0.95     30456\n",
      "          1       0.34      0.86      0.48      1543\n",
      "\n",
      "avg / total       0.96      0.91      0.93     31999\n",
      "\n",
      "[[27851  2605]\n",
      " [  219  1324]]\n",
      "0.953294584234\n",
      "########################################\n",
      "\n",
      "########### identity_hate ####################\n",
      "\n",
      "0.5\n",
      "(2250,) (2250,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96     31719\n",
      "          1       0.09      0.90      0.16       280\n",
      "\n",
      "avg / total       0.99      0.92      0.95     31999\n",
      "\n",
      "[[29140  2579]\n",
      " [   29   251]]\n",
      "0.959060815284\n",
      "########################################\n",
      "\n",
      "0.960770115759\n"
     ]
    }
   ],
   "source": [
    "roc_scores = []\n",
    "for label in ['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']:\n",
    "    print(\"########### %s ####################\\n\"%(label))\n",
    "    y_train_bal, X_train_bal, y_test, X_test = build_balanced_classifier(train, test, label)\n",
    "    clf = text_clf.fit(X=X_train_bal, y=y_train_bal)\n",
    "    y_ = clf.predict_proba(X_test)\n",
    "    y_bin = y_[:,1] > 0.5\n",
    "    print(classification_report(y_test, y_bin))\n",
    "    print(confusion_matrix(y_test, y_bin))\n",
    "    roc = roc_auc_score(y_test, y_[:, 1])\n",
    "    print(roc)\n",
    "    roc_scores.append(roc)\n",
    "    print(\"########################################\\n\")\n",
    "print(sum(roc_scores)/len(roc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = submission[['comment_text']].as_matrix().reshape((-1,))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### toxic ####################\n",
      "\n",
      "0.5\n",
      "(30588,) (30588,)\n",
      "########### severe_toxic ####################\n",
      "\n",
      "0.5\n",
      "(3190,) (3190,)\n",
      "########### obscene ####################\n",
      "\n",
      "0.5\n",
      "(16898,) (16898,)\n",
      "########### threat ####################\n",
      "\n",
      "0.5\n",
      "(956,) (956,)\n",
      "########### insult ####################\n",
      "\n",
      "0.5\n",
      "(15754,) (15754,)\n",
      "########### identity_hate ####################\n",
      "\n",
      "0.5\n",
      "(2810,) (2810,)\n"
     ]
    }
   ],
   "source": [
    "#Create a model on full data set:\n",
    "results = {}\n",
    "for label in ['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']:\n",
    "    print(\"########### %s ####################\\n\"%(label))\n",
    "    y_train_bal, X_train_bal, y_test, X_test = build_balanced_classifier(train = data, test = None, label = label)\n",
    "    clf = text_clf.fit(X=X_train_bal, y=y_train_bal)\n",
    "    y_ = clf.predict_proba(X_submission)\n",
    "    results[label] = y_[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['toxic', 'severe_toxic', 'obscene','threat', 'insult', 'identity_hate']\n",
    "for  label in LABELS:\n",
    "    submission[label] = results[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  \\\n",
      "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
      "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
      "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
      "\n",
      "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
      "0  0.996461      0.930104  0.991982  0.886851  0.988576       0.958470  \n",
      "1  0.098059      0.119510  0.091759  0.190254  0.098779       0.179650  \n",
      "2  0.214485      0.122433  0.182039  0.221707  0.185183       0.207432  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ \"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\",\n",
       "       '== From RfC == \\n\\n The title is fine as it is, IMO.',\n",
       "       '\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"'], dtype=object)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submission[0:3])\n",
    "submission[0:3]['comment_text'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.996461</td>\n",
       "      <td>0.930104</td>\n",
       "      <td>0.991982</td>\n",
       "      <td>0.886851</td>\n",
       "      <td>0.988576</td>\n",
       "      <td>0.958470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.098059</td>\n",
       "      <td>0.119510</td>\n",
       "      <td>0.091759</td>\n",
       "      <td>0.190254</td>\n",
       "      <td>0.098779</td>\n",
       "      <td>0.179650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.214485</td>\n",
       "      <td>0.122433</td>\n",
       "      <td>0.182039</td>\n",
       "      <td>0.221707</td>\n",
       "      <td>0.185183</td>\n",
       "      <td>0.207432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.064713</td>\n",
       "      <td>0.076143</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>0.170788</td>\n",
       "      <td>0.070593</td>\n",
       "      <td>0.076639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.245505</td>\n",
       "      <td>0.200181</td>\n",
       "      <td>0.374682</td>\n",
       "      <td>0.266965</td>\n",
       "      <td>0.363224</td>\n",
       "      <td>0.199277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.996461      0.930104  0.991982  0.886851  0.988576   \n",
       "1  0000247867823ef7  0.098059      0.119510  0.091759  0.190254  0.098779   \n",
       "2  00013b17ad220c46  0.214485      0.122433  0.182039  0.221707  0.185183   \n",
       "3  00017563c3f7919a  0.064713      0.076143  0.067159  0.170788  0.070593   \n",
       "4  00017695ad8997eb  0.245505      0.200181  0.374682  0.266965  0.363224   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.958470  \n",
       "1       0.179650  \n",
       "2       0.207432  \n",
       "3       0.076639  \n",
       "4       0.199277  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [\"id\"]+LABELS\n",
    "print(headers)\n",
    "submission[headers].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[headers].to_csv(\"./input/submissions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
